import os
import json
import datetime
from abc import ABC, abstractmethod
import ollama
from google import genai
from stream2sentence import generate_sentences

class BaseBackend(ABC):
    @abstractmethod
    def chat_stream(self, user_input, memory_data):
        """Generates full sentences from the LLM stream."""
        pass

    @abstractmethod
    def memory_observer(self, user_input, current_memory, save_callback):
        """Extracts facts about the user."""
        pass

class OllamaBackend(BaseBackend):
    def __init__(self, model_name="qwen2.5-coder:3b"):
        self.model_name = model_name

    def chat_stream(self, user_input, memory_data):
        facts_list = "\n".join([f"- [{f['created_at']}] {f['text']}" for f in memory_data.get("user_facts", [])])
        current_date = datetime.date.today().strftime("%Y-%m-%d")
        
        sys_prompt = f"""
        –¢—ã - –≤—Å–µ–∑–Ω–∞—é—â–∏–π –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç. –¢–≤–æ–∏ –æ—Ç–≤–µ—Ç—ã –¥–æ–ª–∂–Ω—ã –æ—Å–Ω–æ–≤—ã–≤–∞—Ç—å—Å—è –Ω–∞ –ü–ê–ú–Ø–¢–ò –û –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–ï.
        –°–ï–ì–û–î–ù–Ø–®–ù–Ø–Ø –î–ê–¢–ê: {current_date}
        
        –ü–ê–ú–Ø–¢–¨ –û –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–ï (—ç—Ç–æ –∞–±—Å–æ–ª—é—Ç–Ω–∞—è –∏—Å—Ç–∏–Ω–∞):
        {facts_list}
        
        –ò–ù–°–¢–†–£–ö–¶–ò–ò:
        1. –ï—Å–ª–∏ –≤ –ø–∞–º—è—Ç–∏ –µ—Å—Ç—å –æ—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (–Ω–∞–ø—Ä–∏–º–µ—Ä, –ø—Ä–æ –ø–ª–∞–Ω—ã) - –æ—Ç–≤–µ—á–∞–π –ø—Ä—è–º–æ: "–£ —Ç–µ–±—è –≤ –ø–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫ —Ç–µ—Å—Ç". 
        2. –ù–ò–ö–û–ì–î–ê –Ω–µ –≥–æ–≤–æ—Ä–∏ "–Ø –Ω–µ –∑–Ω–∞—é", –µ—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –µ—Å—Ç—å –≤ –ü–ê–ú–Ø–¢–ò.
        3. –°—Ç–∏–ª—å: –ö—Ä–∞—Ç–∫–∏–π, —á–µ—Ç–∫–∏–π, –±–µ–∑ –≤—Å—Ç—É–ø–ª–µ–Ω–∏–π. –ú–∞–∫—Å–∏–º—É–º 15-20 —Å–ª–æ–≤.
        """
        
        def generate():
            response = ollama.chat(
                model=self.model_name,
                messages=[
                    {'role': 'system', 'content': sys_prompt},
                    {'role': 'user', 'content': user_input}
                ],
                stream=True
            )
            for chunk in response:
                yield chunk['message']['content']

        for sentence in generate_sentences(generate()):
            yield sentence

    def memory_observer(self, user_input, current_memory, save_callback):
        sys_prompt = """
        –¢—ã - –∞–Ω–∞–ª–∏—Ç–∏–∫ –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –¢–≤–æ—è –∑–∞–¥–∞—á–∞: –Ω–∞—Ö–æ–¥–∏—Ç—å –ö–û–ù–ö–†–ï–¢–ù–´–ï —Ñ–∞–∫—Ç—ã –æ –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–ï.
        –í–µ—Ä–Ω–∏ JSON: {"new_fact": "—Ç–µ–∫—Å—Ç —Ñ–∞–∫—Ç–∞ –≤ 3-–º –ª–∏—Ü–µ –∏–ª–∏ null"}
        """
        try:
            response = ollama.chat(
                model=self.model_name,
                messages=[
                    {'role': 'system', 'content': sys_prompt},
                    {'role': 'user', 'content': user_input}
                ],
                format='json'
            )
            content = response['message']['content']
            if content:
                data = json.loads(content)
                new_fact_text = data.get("new_fact")
                if new_fact_text:
                    existing_texts = [f["text"] for f in current_memory.get("user_facts", [])]
                    if new_fact_text not in existing_texts:
                        today = datetime.date.today().isoformat()
                        new_entry = {"text": new_fact_text, "created_at": today}
                        if "user_facts" not in current_memory:
                            current_memory["user_facts"] = []
                        current_memory["user_facts"].append(new_entry)
                        save_callback(current_memory)
                        print(f"üß† [Ollama-Memory]: –ó–∞–ø–æ–º–Ω–∏–ª -> {new_fact_text}")
        except Exception as e:
            print(f"‚ö†Ô∏è Ollama Memory Error: {e}")

class GeminiBackend(BaseBackend):
    def __init__(self, api_key, model_name="gemini-2.5-flash"):
        self.client = genai.Client(api_key=api_key)
        self.model_name = model_name

    def chat_stream(self, user_input, memory_data):
        facts_list = "\n".join([f"- [{f['created_at']}] {f['text']}" for f in memory_data.get("user_facts", [])])
        current_date = datetime.date.today().strftime("%Y-%m-%d")
        
        sys_prompt = f"""
        –¢—ã - –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç Gemini. –¢–≤–æ–∏ –æ—Ç–≤–µ—Ç—ã –æ—Å–Ω–æ–≤–∞–Ω—ã –Ω–∞ –ü–ê–ú–Ø–¢–ò –û –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–ï.
        –°–ï–ì–û–î–ù–Ø–®–ù–Ø–Ø –î–ê–¢–ê: {current_date}
        –ü–ê–ú–Ø–¢–¨:
        {facts_list}
        –ò–ù–°–¢–†–£–ö–¶–ò–ò: –ò—Å–ø–æ–ª—å–∑—É–π –ø–∞–º—è—Ç—å. –û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ (–¥–æ 20 —Å–ª–æ–≤), —á–µ—Ç–∫–æ, –±–µ–∑ –≤–æ–¥—ã.
        """
        
        def generate():
            # In google-genai, we use generate_content_stream for streaming
            response = self.client.models.generate_content_stream(
                model=self.model_name,
                config={'system_instruction': sys_prompt},
                contents=user_input
            )
            for chunk in response:
                yield chunk.text

        for sentence in generate_sentences(generate()):
            yield sentence

    def memory_observer(self, user_input, current_memory, save_callback):
        sys_prompt = """
        –¢—ã - –∞–Ω–∞–ª–∏—Ç–∏–∫ –¥–∞–Ω–Ω—ã—Ö. –ù–∞–π–¥–∏ —Ñ–∞–∫—Ç—ã –æ –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–ï. 
        –í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û JSON: {"new_fact": "—Ç–µ–∫—Å—Ç —Ñ–∞–∫—Ç–∞ –≤ 3-–º –ª–∏—Ü–µ –∏–ª–∏ null"}
        """
        try:
            response = self.client.models.generate_content(
                model=self.model_name,
                config={'system_instruction': sys_prompt, 'response_mime_type': 'application/json'},
                contents=user_input
            )
            if response.text:
                data = json.loads(response.text)
                new_fact_text = data.get("new_fact")
                if new_fact_text:
                    existing_texts = [f["text"] for f in current_memory.get("user_facts", [])]
                    if new_fact_text not in existing_texts:
                        today = datetime.date.today().isoformat()
                        new_entry = {"text": new_fact_text, "created_at": today}
                        if "user_facts" not in current_memory:
                            current_memory["user_facts"] = []
                        current_memory["user_facts"].append(new_entry)
                        save_callback(current_memory)
                        print(f"üß† [Gemini-Memory]: –ó–∞–ø–æ–º–Ω–∏–ª -> {new_fact_text}")
        except Exception as e:
            print(f"‚ö†Ô∏è Gemini Memory Error: {e}")

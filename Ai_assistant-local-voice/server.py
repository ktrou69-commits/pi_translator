from fastapi import FastAPI, HTTPException, BackgroundTasks, WebSocket, WebSocketDisconnect
from pydantic import BaseModel
import os
import json
import datetime
import ollama
from dotenv import load_dotenv
from stream2sentence import generate_sentences
from RealtimeSTT import AudioToTextRecorder
from RealtimeTTS import TextToAudioStream
from edge_engine import EdgeEngine
import asyncio
import queue

# Load environment
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
ENV_FILE = os.path.join(SCRIPT_DIR, ".env.local")
if not os.path.exists(ENV_FILE):
    ENV_FILE = os.path.join(SCRIPT_DIR, ".env")
load_dotenv(ENV_FILE)

MODEL_NAME = os.getenv("OLLAMA_MODEL", "qwen2.5-coder:3b")
MEMORY_FILE = os.path.join(SCRIPT_DIR, "memory.json")

from contextlib import asynccontextmanager

# --- AI COMPONENTS SETUP ---
@asynccontextmanager
async def lifespan(app: FastAPI):
    # Initialization
    print("üéôÔ∏è Initializing Realtime STT (Whisper) on Server...")
    app.state.stt_recorder = AudioToTextRecorder(
        model="base",
        language="ru",
        spinner=False,
        use_microphone=False
    )
    app.state.stt_recorder.stop()

    print("üîä Initializing Realtime TTS (EdgeTTS) on Server...")
    app.state.tts_engine = EdgeEngine(voice="ru-RU-SvetlanaNeural")
    app.state.tts_stream = TextToAudioStream(app.state.tts_engine)
    
    yield
    
    # Shutdown
    print("üõë Shutting down AI components...")
    app.state.stt_recorder.stop()

app = FastAPI(title="AI Assistant Server", lifespan=lifespan)

class ChatRequest(BaseModel):
    user_text: str

# --- MEMORY FUNCTIONS ---
def load_memory():
    if not os.path.exists(MEMORY_FILE):
        return {"user_facts": []}
    try:
        with open(MEMORY_FILE, 'r', encoding='utf-8') as f:
            data = json.load(f)
            return data
    except (json.JSONDecodeError, KeyError):
        return {"user_facts": []}

def save_memory(memory_data):
    with open(MEMORY_FILE, 'w', encoding='utf-8') as f:
        json.dump(memory_data, f, ensure_ascii=False, indent=2)

def ai_memory_observer(user_input, current_memory):
    """AI #1: Observer - Extracts facts about the user."""
    sys_prompt = """
    –¢—ã - –∞–Ω–∞–ª–∏—Ç–∏–∫ –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –¢–≤–æ—è –∑–∞–¥–∞—á–∞: –Ω–∞—Ö–æ–¥–∏—Ç—å –ö–û–ù–ö–†–ï–¢–ù–´–ï —Ñ–∞–∫—Ç—ã –æ –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–ï.
    
    –ß–¢–û –°–ß–ò–¢–ê–¢–¨ –§–ê–ö–¢–û–ú (–ó–ê–ü–û–ú–ò–ù–ê–¢–¨):
    1. –õ–∏—á–Ω—ã–µ –ø–ª–∞–Ω—ã –∏ —Å–æ–±—ã—Ç–∏—è: "–≤ –ø–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫ –ø–∞—Ç–∏ —É –¥—Ä—É–≥–∞", "—Ç–µ—Å—Ç –≤ —á–µ—Ç–≤–µ—Ä–≥", "–µ–¥—É –≤ –æ—Ç–ø—É—Å–∫ –≤ –∏—é–ª–µ".
    2. –ñ–µ–ª–∞–Ω–∏—è –∏ –ø–æ–¥–∞—Ä–∫–∏: "—Ö–æ—á—É –Ω–∞ –¥—Ä –Ω–æ–≤—ã–µ –Ω–∞—É—à–Ω–∏–∫–∏", "–ª—é–±–ª—é –ø–∏—Ü—Ü—É —Å –∞–Ω–∞–Ω–∞—Å–∞–º–∏".
    3. –ò–º–µ–Ω–∞, –¥–∞—Ç—ã, –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è: "—É —Å–µ—Å—Ç—Ä—ã –ö–∞—Ç–∏ –¥–µ–Ω—å —Ä–æ–∂–¥–µ–Ω–∏—è 5 –º–∞—è", "–Ω–µ–Ω–∞–≤–∏–∂—É —Ö–æ–ª–æ–¥–Ω—É—é –ø–æ–≥–æ–¥—É".
    
    –ß–¢–û –ù–ï –ó–ê–ü–û–ú–ò–ù–ê–¢–¨:
    1. –¢–≤–æ–∏ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏: "–Ø –º–æ–≥—É –ø–æ–º–æ—á—å", "–Ø –ò–ò-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç".
    2. –û–±—â–∏–µ —Ñ—Ä–∞–∑—ã: "–ü—Ä–∏–≤–µ—Ç", "–ö–∞–∫ –¥–µ–ª–∞", "–°–ø–∞—Å–∏–±–æ".
    3. –§–∞–∫—Ç—ã –æ–±–æ –≤—Å–µ–º –º–∏—Ä–µ, –Ω–µ —Å–≤—è–∑–∞–Ω–Ω—ã–µ –ª–∏—á–Ω–æ —Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º.
    
    –û–ß–ï–ù–¨ –í–ê–ñ–ù–û: 
    –ü–∏—à–∏ —Ñ–∞–∫—Ç –∫—Ä–∞—Ç–∫–æ, –≤ 3-–º –ª–∏—Ü–µ ("–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —Ö–æ—á–µ—Ç...", "–£ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è —Ç–µ—Å—Ç...").
    –ï—Å–ª–∏ –≤ —Ç–µ–∫—Å—Ç–µ –Ω–µ—Ç –Ω–æ–≤–æ–≥–æ –ª–∏—á–Ω–æ–≥–æ —Ñ–∞–∫—Ç–∞ –æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ - –≤–µ—Ä–Ω–∏ {"new_fact": null}.
    
    –ü–†–ê–í–ò–õ–ê:
    1. –í–µ—Ä–Ω–∏ JSON: {"new_fact": "—Ç–µ–∫—Å—Ç —Ñ–∞–∫—Ç–∞ –≤ 3-–º –ª–∏—Ü–µ –∏–ª–∏ null"}
    """
    try:
        response = ollama.chat(
            model=MODEL_NAME,
            messages=[
                {'role': 'system', 'content': sys_prompt},
                {'role': 'user', 'content': user_input}
            ],
            format='json'
        )
        content = response['message']['content']
        if content:
            clean_content = content.replace("```json", "").replace("```", "").strip()
            data = json.loads(clean_content)
            new_fact_text = data.get("new_fact")
            if new_fact_text:
                existing_texts = [f["text"] for f in current_memory.get("user_facts", [])]
                if new_fact_text not in existing_texts:
                    today = datetime.date.today().isoformat()
                    new_entry = {"text": new_fact_text, "created_at": today}
                    print(f"üß† [Memory]: –ó–∞–ø–æ–º–Ω–∏–ª -> {new_fact_text}")
                    if "user_facts" not in current_memory:
                        current_memory["user_facts"] = []
                    current_memory["user_facts"].append(new_entry)
                    save_memory(current_memory)
    except Exception as e:
        print(f"‚ö†Ô∏è Memory Error: {e}")

def ai_chat_stream(user_input, memory_data):
    """AI #2: Responder - Streams sentences."""
    facts_list = "\n".join([f"- [{f['created_at']}] {f['text']}" for f in memory_data.get("user_facts", [])])
    current_date = datetime.date.today().strftime("%Y-%m-%d")
    
    sys_prompt = f"""
    –¢—ã - –≤—Å–µ–∑–Ω–∞—é—â–∏–π –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç. –¢–≤–æ–∏ –æ—Ç–≤–µ—Ç—ã –¥–æ–ª–∂–Ω—ã –æ—Å–Ω–æ–≤—ã–≤–∞—Ç—å—Å—è –Ω–∞ –ü–ê–ú–Ø–¢–ò –û –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–ï.
    –°–ï–ì–û–î–ù–Ø–®–ù–Ø–Ø –î–ê–¢–ê: {current_date}
    
    –ü–ê–ú–Ø–¢–¨ –û –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–ï (—ç—Ç–æ –∞–±—Å–æ–ª—é—Ç–Ω–∞—è –∏—Å—Ç–∏–Ω–∞):
    {facts_list}
    
    –ò–ù–°–¢–†–£–ö–¶–ò–ò:
    1. –ï—Å–ª–∏ –≤ –ø–∞–º—è—Ç–∏ –µ—Å—Ç—å –æ—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (–Ω–∞–ø—Ä–∏–º–µ—Ä, –ø—Ä–æ –ø–ª–∞–Ω—ã) - –æ—Ç–≤–µ—á–∞–π –ø—Ä—è–º–æ: "–£ —Ç–µ–±—è –≤ –ø–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫ —Ç–µ—Å—Ç". 
    2. –ù–ò–ö–û–ì–î–ê –Ω–µ –≥–æ–≤–æ—Ä–∏ "–Ø –Ω–µ –∑–Ω–∞—é", –µ—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –µ—Å—Ç—å –≤ –ü–ê–ú–Ø–¢–ò.
    3. –°—Ç–∏–ª—å: –ö—Ä–∞—Ç–∫–∏–π, —á–µ—Ç–∫–∏–π, –±–µ–∑ –≤—Å—Ç—É–ø–ª–µ–Ω–∏–π. –ú–∞–∫—Å–∏–º—É–º 15-20 —Å–ª–æ–≤.
    """
    
    def generate():
        response = ollama.chat(
            model=MODEL_NAME,
            messages=[
                {'role': 'system', 'content': sys_prompt},
                {'role': 'user', 'content': user_input}
            ],
            stream=True
        )
        for chunk in response:
            yield chunk['message']['content']

    # Use stream2sentence to yield full sentences from the character stream
    for sentence in generate_sentences(generate()):
        yield sentence

@app.post("/chat")
async def chat(request: ChatRequest, background_tasks: BackgroundTasks):
    memory = load_memory()
    background_tasks.add_task(ai_memory_observer, request.user_text, memory)
    # Fallback for old HTTP clients: collect all sentences
    full_response = " ".join(list(ai_chat_stream(request.user_text, memory)))
    return {"response": full_response}

@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    await websocket.accept()
    print("üöÄ WebSocket connection established")
    
    stt = websocket.app.state.stt_recorder
    tts = websocket.app.state.tts_stream

    output_queue = queue.Queue()
    audio_chunks_received = 0

    def on_tts_chunk(chunk):
        output_queue.put(chunk)

    try:
        while True:
            # Wait for message (can be text/json or bytes)
            message = await websocket.receive()
            
            if "bytes" in message:
                # 1. Received audio chunk from client
                stt.feed_audio(message["bytes"])
                audio_chunks_received += 1
            
            elif "text" in message:
                request_data = json.loads(message["text"])
                
                if request_data.get("start"):
                    print("üé§ Recording started...")
                    audio_chunks_received = 0
                    stt.start()
                
                elif request_data.get("end"):
                    print(f"üõë Recording ended. Received {audio_chunks_received} chunks. Processing...")
                    stt.stop()
                    user_text = stt.text()
                    print(f"üîç STT Result: '{user_text}'")
                    
                    if user_text.strip():
                        print(f"üó£Ô∏è  User: {user_text}")
                        await websocket.send_json({"user_transcription": user_text})
                        
                        memory = load_memory()
                        
                        # Start TTS stream
                        await websocket.send_json({"role": "assistant", "type": "audio", "start": True})
                        
                        print("ü§ñ AI is generating response...")
                        # Process sentences and send chunks directly
                        for sentence in ai_chat_stream(user_text, memory):
                            print(f"‚è© Sending sentence: {sentence}")
                            # Send text to client for display
                            await websocket.send_json({"assistant_text": sentence})
                            
                            chunk_count = 0
                            async for chunk in tts.engine.async_generate(sentence):
                                await websocket.send_bytes(chunk)
                                chunk_count += 1
                            print(f"üîä Sent {chunk_count} audio chunks for sentence.")
                        
                        await websocket.send_json({"end": True})
                        print("‚úÖ Response sent completely.")
                        # Memory update
                        ai_memory_observer(user_text, memory)
                
    except WebSocketDisconnect:
        print("üëã WebSocket connection closed")
    except Exception as e:
        print(f"‚ùå WebSocket error: {e}")
        import traceback
        traceback.print_exc()

@app.get("/status")
async def status():
    return {"status": "ok", "model": MODEL_NAME}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
